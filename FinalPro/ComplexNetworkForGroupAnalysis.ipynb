{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sc\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import networkx as nx \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import json\n",
    "import random\n",
    "c2a = np.loadtxt('c2a_small_sub.txt')\n",
    "a2q = np.loadtxt('a2q_small_sub.txt')\n",
    "c2q = np.loadtxt('c2q_small_sub.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------#\n",
    "#                     Function Part                        #\n",
    "#----------------------------------------------------------#\n",
    "def create_graph(data):\n",
    "    G = nx.Graph()\n",
    "    edges = data[:,0:2].copy()\n",
    "    G.add_edges_from(edges)\n",
    "    return G\n",
    "def construct_graph(G_c2a,G_a2q,G_c2q):\n",
    "    node_c2a = G_c2a.nodes\n",
    "    node_a2q = G_a2q.nodes\n",
    "    node_c2q = G_c2q.nodes\n",
    "\n",
    "    node = list(node_c2a&node_a2q&node_c2q)\n",
    "\n",
    "    g_c2a = G_c2a.subgraph(node)\n",
    "    g_a2q = G_a2q.subgraph(node)\n",
    "    g_c2q = G_c2q.subgraph(node)\n",
    "    return g_c2a,g_a2q,g_c2q\n",
    "\n",
    "def rec_dd():\n",
    "    return defaultdict(rec_dd)\n",
    "def neighbors_in_each_temporal_network(dict_data):\n",
    "    temp = dict_data.values()\n",
    "    temp  =  [x for x in temp if x]\n",
    "    neighbors_set = list(itertools.chain.from_iterable(temp))\n",
    "    return neighbors_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18965, 3) (4326, 3) (4326, 3) (27683, 3)\n",
      "Combined Graph:number of edges: 5208 /number of nodes: 758 /is connected or not?: True\n",
      "Graph:c2a number of edges: 2557 /number of nodes: 657 /is connected or not?: False\n",
      "Graph:a2q number of edges: 2716 /number of nodes: 657 /is connected or not?: False\n",
      "Graph:c2q number of edges: 1355 /number of nodes: 657 /is connected or not?: False\n",
      "Connected Graph:number of edges: 5208 /number of nodes: 758 /is connected or not?: True\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------#\n",
    "#  Combining 3 Layers to Construct Completed Network       #\n",
    "#----------------------------------------------------------#\n",
    "Total_1500_nodes = np.concatenate((c2a, a2q), axis=0)\n",
    "Total_1500_nodes = np.concatenate((Total_1500_nodes,c2q),axis=0)\n",
    "print(c2a.shape,c2q.shape, c2q.shape,Total_1500_nodes.shape, )\n",
    "total_network = create_graph(Total_1500_nodes)\n",
    "#  examine if the combined network is right\n",
    "c2a_graph = create_graph(c2a)\n",
    "a2q_graph = create_graph(a2q)\n",
    "c2q_graph = create_graph(c2q)\n",
    "# to get common nodes for each layer\n",
    "g_c2a, g_a2q, g_c2q = construct_graph(c2a_graph,a2q_graph,c2q_graph)\n",
    "\n",
    "print('Combined Graph:''number of edges:',total_network.number_of_edges(),'/number of nodes:',total_network.number_of_nodes(),'/is connected or not?:',nx.is_connected(total_network))\n",
    "print('Graph:c2a','number of edges:',g_c2a.number_of_edges(),'/number of nodes:',g_c2a.number_of_nodes(),'/is connected or not?:',nx.is_connected(g_c2a))\n",
    "print('Graph:a2q','number of edges:',g_a2q.number_of_edges(),'/number of nodes:',g_a2q.number_of_nodes(),'/is connected or not?:',nx.is_connected(g_a2q))\n",
    "print('Graph:c2q','number of edges:',g_c2q.number_of_edges(),'/number of nodes:',g_c2q.number_of_nodes(),'/is connected or not?:',nx.is_connected(g_c2q))\n",
    "\n",
    "connected_graph = max(nx.connected_component_subgraphs(total_network), key=len)\n",
    "print('Connected Graph:''number of edges:',connected_graph.number_of_edges(),'/number of nodes:',connected_graph.number_of_nodes(),'/is connected or not?:',nx.is_connected(connected_graph))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------#\n",
    "#                Choose One Centrality                     #\n",
    "#----------------------------------------------------------#\n",
    "# finally we chose closeness considering there r lots of 0 in betweenness \n",
    "# betweenness = nx.betweenness_centrality(connected_graph)\n",
    "closeness = nx.closeness_centrality(connected_graph)\n",
    "#----------------------------------------------------------#\n",
    "#      Rank Centrality to Seperate 3 Groups                #\n",
    "#----------------------------------------------------------#\n",
    "# if we use percentile then there is no need to use 'sorted' part\n",
    "# sorted_betweeness = sorted(betweenness.items(), key=operator.itemgetter(1))\n",
    "# sorted_closeness = sorted(closeness.items(), key=operator.itemgetter(1))\n",
    "# print(sorted_closeness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------#\n",
    "#                   Processing Timestamp                   #\n",
    "#----------------------------------------------------------#\n",
    "sorted_timestamp = sorted(Total_1500_nodes, key=lambda entry: entry[2])\n",
    "timestamp_sorted_data = pd.DataFrame(sorted_timestamp)\n",
    "# df[list(\"ABCD\")] = df[list(\"ABCD\")].astype(int)\n",
    "# lower_quantile, lower_medium_quantile,upper_medium_quantile,upper_quantile = timestamp_sorted_data[2].quantile([.25, .50,.75,1])\n",
    "# print(lower_quantile,lower_medium_quantile,upper_medium_quantile,upper_quantile)\n",
    "\n",
    "# use index of sorted data to seperate the network into four temporal networks and create corresponding graphs\n",
    "temp1 = timestamp_sorted_data.iloc[:6920, :]\n",
    "temp2 = timestamp_sorted_data.iloc[6921:13840, :]\n",
    "temp3 = timestamp_sorted_data.iloc[13841:20760, :]\n",
    "temp4 = timestamp_sorted_data.iloc[20761:27683, :]\n",
    "\n",
    "temp1_graph = create_graph(temp1.values)\n",
    "temp2_graph = create_graph(temp2.values)\n",
    "temp3_graph = create_graph(temp3.values)\n",
    "temp4_graph = create_graph(temp4.values)\n",
    "graph_list= [temp1_graph,temp2_graph,temp3_graph,temp4_graph]\n",
    "\n",
    "temp1_nodes = temp1_graph.nodes()\n",
    "temp2_nodes = temp2_graph.nodes()\n",
    "temp3_nodes = temp3_graph.nodes()\n",
    "temp4_nodes = temp4_graph.nodes()\n",
    "graph_nodes_list = [temp1_nodes, temp2_nodes, temp3_nodes,temp4_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------#\n",
    "#       How to Select Nodes from Different Group           #\n",
    "#----------------------------------------------------------#\n",
    "# we try to divide all nodes into three parts: 1.active users 2. normal users 3. inactive users\n",
    "# nodes with lower closeness should be the inactive users group\n",
    "# nodes with higher closeness should be the active users group\n",
    "# clossness of normal users is between it of inactive users and active users\n",
    "\n",
    "# find the dividing line of groups by the value of clossness\n",
    "t1 = np.percentile(list(closeness.values()),33)\n",
    "t2 = np.percentile(list(closeness.values()),67)\n",
    "print(t1, t2)\n",
    "#t1, t2: 0.33184435421062075 0.36587723537941036\n",
    "\n",
    "# one question here: how many nodes should we choose for each group?\n",
    " \n",
    "# following code trys to analyze the distribution of commone users in four temporal graph\n",
    "# then I find every node is active node if we try to find common users in four temporal graphs\n",
    "# which means all closeness is bigger than t2\n",
    "# so I should use other metheds to select nodes\n",
    "\n",
    "temp_graph_nodes = None\n",
    "for graph in graph_list:\n",
    "    if temp_graph is None:\n",
    "        temp_graph_nodes = set(graph.nodes())\n",
    "    else:\n",
    "        temp_graph_nodes = set(graph.nodes()) & temp_graph_nodes\n",
    "# print(temp_graph)\n",
    "# set(temp1_graph.nodes()\n",
    "\n",
    "# candidate_node_dict = defaultdict(list)\n",
    "# for node in temp_graph:\n",
    "#     temp_graph_closeness = nx.closeness_centrality(connected_graph, node)\n",
    "#     if temp_graph_closeness <= t1:\n",
    "#         label = 0 # is inactive user\n",
    "#     elif temp_graph_closeness> t2:\n",
    "#         label = 2 # active user\n",
    "#     else:\n",
    "#         label = 1 # normal user\n",
    "#     candidate_node_dict[label].append(node)\n",
    "# print(candidate_node_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249 250 259\n"
     ]
    }
   ],
   "source": [
    "#finally I decide to create a criterion to find reasonable number of nodes in each group\n",
    "candidate_node_dict = defaultdict(list)\n",
    "all_active_users_set = []\n",
    "all_inactive_users_set = []\n",
    "all_normal_users_set = []\n",
    "\n",
    "for node in connected_graph.nodes():\n",
    "    connected_graph_closeness = nx.closeness_centrality(connected_graph, node)\n",
    "    if connected_graph_closeness <= t1:\n",
    "        all_inactive_users_set.append(node) # is inactive user\n",
    "    elif connected_graph_closeness> t2:\n",
    "        all_active_users_set.append(node) # active user\n",
    "    else:\n",
    "        all_normal_users_set.append(node)# normal user\n",
    "print(len(all_active_users_set), len(all_inactive_users_set), len(all_normal_users_set))\n",
    "#250 249 259\n",
    "\n",
    "group_nodes_list = [all_active_users_set, all_inactive_users_set, all_normal_users_set]\n",
    "graph_nodes_list = [temp1_nodes, temp2_nodes, temp3_nodes,temp4_nodes]\n",
    "\n",
    "user_score = {}\n",
    "for user_set in group_nodes_list:\n",
    "    for user_index, user in zip(range(1, len(user_set)+1),user_set):\n",
    "        score = 0\n",
    "        for temp_node_set in graph_nodes_list:\n",
    "                if user in temp_node_set:\n",
    "                    score = score + 1\n",
    "        user_score[user] = score\n",
    "# print(user_score)\n",
    "\n",
    "for index, user_set in zip(range(1,len(group_nodes_list)+1), group_nodes_list):\n",
    "    score_for_each_group = dict((key, user_score[key]) for key in user_set)\n",
    "    average_score = sum(score_for_each_group.values())/float(len(score_for_each_group))\n",
    "    print('index:',index, 'average_score:',average_score)\n",
    "#     print('sum:',sum(score_for_each_group), 'length:' , len(score_for_each_group))\n",
    "# all_active_users_set: 1 average_score: 3.3654618473895583\n",
    "# all_inactive_users_set: 2 average_score: 1.916\n",
    "# all_normal_users_set: 3 average_score: 2.7606177606177607\n",
    "\n",
    "# basic math here:\n",
    "# 1 average_score* 2 average_score* 3 average_score almost equals 18\n",
    "# so for acitve users I choose 18/3.37 = 5\n",
    "# as for inactive users 18/1.9 = 10\n",
    "# as for normal users 18/2.76 = 6\n",
    "# so I randomly pick 5, 10, 6 users from active_user_set, inacitve_user_set normal_user_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74601.0, 91607.0, 42769.0, 82686.0, 19100.0] [87942.0, 92957.0, 111833.0, 15562.0, 149825.0, 58997.0, 47878.0, 15816.0, 58549.0, 4771.0] [12881.0, 425.0, 79853.0, 94162.0, 894.0, 10693.0]\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------#\n",
    "#       Selecting Nodes from Different Group               #\n",
    "#----------------------------------------------------------#\n",
    "selected_active_users = random.sample(all_active_users_set, 5)\n",
    "selected_inactive_users = random.sample(all_inactive_users_set, 10)\n",
    "selected_normal_users = random.sample(all_normal_users_set, 6)\n",
    "# print(selected_active_users, selected_inactive_users, selected_normal_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "# Constructing Subgraph for Selected nodes and their Neighbors   #\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "#----------------------------------------------------------#\n",
    "#               For 1 HOP Neighbors                        #\n",
    "#----------------------------------------------------------#\n",
    "\n",
    "selected_userset_list = [selected_active_users, selected_inactive_users, selected_normal_users]\n",
    "neighbor_dict = rec_dd()\n",
    "for network_index, temporal_network in zip(range(1, len(graph_list)+1), graph_list):\n",
    "    for user_set_index, user_set in zip(range(1, len(selected_userset_list)+1),selected_userset_list):\n",
    "        for i, node in zip(range(1, len(user_set)+1), user_set):\n",
    "            try:\n",
    "                neighbor_list = list(nx.all_neighbors(temporal_network, node))\n",
    "            except Exception as e:\n",
    "#                 print(e)\n",
    "                neighbor_list = []\n",
    "            neighbor_dict[network_index][user_set_index][i] = neighbor_list\n",
    "\n",
    "# print(json.dumps(neighbor_dict, indent = 4))     \n",
    "\n",
    "# three groups, four temporal networks, so 12 set.\n",
    "\n",
    "active_users_neighbors_in_temp1 = neighbors_in_each_temporal_network(neighbor_dict[1][1])\n",
    "inactive_users_neighbors_in_temp1 = neighbors_in_each_temporal_network(neighbor_dict[1][2])\n",
    "normal_users_neighbors_in_temp1 = neighbors_in_each_temporal_network(neighbor_dict[1][3])\n",
    "\n",
    "active_users_neighbors_in_temp2 = neighbors_in_each_temporal_network(neighbor_dict[2][1])\n",
    "inactive_users_neighbors_in_temp2 = neighbors_in_each_temporal_network(neighbor_dict[2][2])\n",
    "normal_users_neighbors_in_temp2 = neighbors_in_each_temporal_network(neighbor_dict[2][3])\n",
    "\n",
    "active_users_neighbors_in_temp3 = neighbors_in_each_temporal_network(neighbor_dict[3][1])\n",
    "inactive_users_neighbors_in_temp3 = neighbors_in_each_temporal_network(neighbor_dict[3][2])\n",
    "normal_users_neighbors_in_temp3 = neighbors_in_each_temporal_network(neighbor_dict[3][3])\n",
    "\n",
    "active_users_neighbors_in_temp4 = neighbors_in_each_temporal_network(neighbor_dict[4][1])\n",
    "inactive_users_neighbors_in_temp4 = neighbors_in_each_temporal_network(neighbor_dict[4][2])\n",
    "normal_users_neighbors_in_temp4 = neighbors_in_each_temporal_network(neighbor_dict[4][3])\n",
    "# print(len(active_users_neighbors_in_temp1),len(inactive_users_neighbors_in_temp3),len(normal_users_neighbors_in_temp4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acitve_users_graph:number of edges: 79 /number of nodes: 27 /is connected or not?: True\n",
      "normal_users_graph:number of edges: 16 /number of nodes: 10 /is connected or not?: False\n",
      "inactive_users_graph:number of edges: 12 /number of nodes: 10 /is connected or not?: False\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------#\n",
    "#             1 HOP Neighbors SubGraph                     #\n",
    "#----------------------------------------------------------#\n",
    "graph_list= [temp1_graph,temp2_graph,temp3_graph,temp4_graph]\n",
    "\n",
    "active_in_temp1_graph =temp1_graph.subgraph(active_users_neighbors_in_temp1)\n",
    "inactive_in_temp1_graph =temp1_graph.subgraph(inactive_users_neighbors_in_temp1)\n",
    "normal_in_temp1_graph =temp1_graph.subgraph(normal_users_neighbors_in_temp1)\n",
    "\n",
    "active_in_temp2_graph =temp2_graph.subgraph(active_users_neighbors_in_temp2)\n",
    "inactive_in_temp2_graph =temp2_graph.subgraph(inactive_users_neighbors_in_temp2)\n",
    "normal_in_temp2_graph =temp2_graph.subgraph(normal_users_neighbors_in_temp2)\n",
    "\n",
    "active_in_temp3_graph =temp3_graph.subgraph(active_users_neighbors_in_temp3)\n",
    "inactive_in_temp3_graph =temp3_graph.subgraph(inactive_users_neighbors_in_temp3)\n",
    "normal_in_temp3_graph =temp3_graph.subgraph(normal_users_neighbors_in_temp3)\n",
    "\n",
    "active_in_temp4_graph =temp4_graph.subgraph(active_users_neighbors_in_temp4)\n",
    "inactive_in_temp4_graph =temp4_graph.subgraph(inactive_users_neighbors_in_temp4)\n",
    "normal_in_temp4_graph =temp4_graph.subgraph(normal_users_neighbors_in_temp4)\n",
    "\n",
    "# random examination \n",
    "print('acitve_users_graph:''number of edges:',active_in_temp1_graph.number_of_edges(),'/number of nodes:',active_in_temp1_graph.number_of_nodes(),'/is connected or not?:',nx.is_connected(active_in_temp1_graph))\n",
    "print('normal_users_graph:''number of edges:',normal_in_temp3_graph.number_of_edges(),'/number of nodes:',normal_in_temp3_graph.number_of_nodes(),'/is connected or not?:',nx.is_connected(normal_in_temp3_graph))\n",
    "print('inactive_users_graph:''number of edges:',inactive_in_temp4_graph.number_of_edges(),'/number of nodes:',inactive_in_temp4_graph.number_of_nodes(),'/is connected or not?:',nx.is_connected(inactive_in_temp4_graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12 12 12\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------#\n",
    "#               1 HOP SubGraph Centrality                  #\n",
    "#----------------------------------------------------------#\n",
    "# 4 centrality for 12 networks: 48\n",
    "\n",
    "neighbors_subgraph_list = [active_in_temp1_graph,inactive_in_temp1_graph,normal_in_temp1_graph,active_in_temp2_graph, inactive_in_temp2_graph,normal_in_temp2_graph,active_in_temp3_graph, inactive_in_temp3_graph,normal_in_temp3_graph,active_in_temp4_graph, inactive_in_temp4_graph,normal_in_temp4_graph,]\n",
    "\n",
    "Degree_data = []\n",
    "Eigenvector_data = []\n",
    "Closeness_data = []\n",
    "Betweenness_data = []\n",
    "for graph_index, different_graph in zip(range(1, len(neighbors_subgraph_list)+1), neighbors_subgraph_list):\n",
    "    Degree_data.append(nx.degree_centrality(different_graph))\n",
    "    Eigenvector_data.append(nx.eigenvector_centrality_numpy(different_graph))\n",
    "    Closeness_data.append( nx.closeness_centrality(different_graph))\n",
    "    Betweenness_data.append( nx.betweenness_centrality(different_graph))\n",
    "    \n",
    "print(len(Degree_data), len(Eigenvector_data), len(Closeness_data), len(Betweenness_data))\n",
    "\n",
    "centrality_data = pd.DataFrame({'Degree': Degree_data})\n",
    "centrality_data['Eigenvector'] = pd.Series(Eigenvector_data)\n",
    "centrality_data['Closeness'] = pd.Series(Closeness_data)\n",
    "centrality_data['Betweenness'] = pd.Series(Betweenness_data)\n",
    "\n",
    "# a = centrality_data.index[0]\n",
    "\n",
    "centrality_data = centrality_data.rename(index={centrality_data.index[0]: 'ActiveInTemp1'})\n",
    "centrality_data = centrality_data.rename(index={centrality_data.index[1]: 'InActiveInTemp1'})\n",
    "centrality_data = centrality_data.rename(index={centrality_data.index[2]: 'NormalInTemp1'})  \n",
    "centrality_data = centrality_data.rename(index={centrality_data.index[3]: 'ActiveInTemp2'})\n",
    "centrality_data = centrality_data.rename(index={centrality_data.index[4]: 'InActiveInTemp2'})\n",
    "centrality_data = centrality_data.rename(index={centrality_data.index[5]: 'NormalInTemp2'})\n",
    "centrality_data = centrality_data.rename(index={centrality_data.index[6]: 'ActiveInTemp3'})\n",
    "centrality_data = centrality_data.rename(index={centrality_data.index[7]: 'InActiveInTemp3'})\n",
    "centrality_data = centrality_data.rename(index={centrality_data.index[8]: 'NormalInTemp3'})                         \n",
    "centrality_data = centrality_data.rename(index={centrality_data.index[9]: 'ActiveInTemp4'})\n",
    "centrality_data = centrality_data.rename(index={centrality_data.index[10]: 'InActiveInTemp4'})\n",
    "centrality_data = centrality_data.rename(index={centrality_data.index[11]: 'NormalInTemp4'})  \n",
    "centrality_data.to_csv('3Group4TempCentralityData.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------#\n",
    "#               For 2 HOP                                  #\n",
    "#----------------------------------------------------------#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
